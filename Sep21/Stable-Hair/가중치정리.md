Stage1

- models/stage1/pytorch_model.bin
      - 역할: bald 변환용 ControlNet(잠재공간) 가중치
      - 사용처: gradio_demo_full.py의 bald_converter에 로드 → remove_hair_pipeline에서 ID 이미지를 “대머리(bald)”로 변환

Stage2
- models/stage2/pytorch_model.bin
      - 역할: 레퍼런스 인코더(ref_unet) 가중치
      - 사용처: ref_unet.from_pretrained(...).load_state_dict(...) → 참조 헤어 이미지에서 특성 추출
  - models/stage2/pytorch_model_1.bin
      - 역할: 어댑터(adapter_injection) 가중치
      - 사용처: adapter_injection(self.pipeline.unet, ...) 후 load_state_dict(...) → 인코더 특성을 UNet에 주입
  - models/stage2/pytorch_model_2.bin
      - 역할: 헤어 전이용 ControlNet(잠재공간) 가중치
      - 사용처: ControlNetModel.from_unet(unet)에 load_state_dict(...) → Stage2 생성 시 구조/컨디션 가이던스 제공



0930
StlyeGan 똑같은 pose일때 가장 안정적임
ref 헤어 포즈가 다르면... 포즈 맞추다 보면 머리 방향이 달라짐

그렇다면 ref 포즈 바꿔주는 환경에서는 diffusion이 나음

source 이미지에서는 stylegan 이 잘 되는 경우에만 적요됨
다른 포즈 일 때는 영상 자체는 StyleGAN으로 하는 게 좋고, source image 이미지가 이상할 때는 그거를 맞춰주는 과정을 diffusion 

diffusion을 쓰는 거가 

StyleGAN이 못하는게 ref pose 너무 다를 때
Diffusion 못하는게 style contest 잘 생성함. ref 헤어 입히는 게 

pose alignment를 Diffusion으로 대체?

10/02
latent ControlNet의 size가 훨씬 작음

=> Original ControlNet 대비 Latent Controlnet
이런 경우는 LAtent ControlNet이 훨씬 낫다. 
근거는 이 논문이다.

메모리를 초과하는 경우.
페이징이 안돌

A100으로 한 번돌려보고 시간 체크.

3가지 방법에 대하여 
다 잘 되는 거
어떤거는 잘 되는 거
GAN은 포즈가 다르면 잘 안내

source-ref 포즈 차이

풍성한 머리에서 풍성하지 않은 머리 

반대 케이스는 문제는 덜 생기는 데 bridge 없어진다는 거

각 논묺마다 limitation 정리해봐
실재사람 데이터도 확인

머리 사이즈부터 다름. 

알록달록한 

hair-fusion 이랑 stable-hair에서 명확하게 안되는 거 정리!



