Stage1

- models/stage1/pytorch_model.bin
      - 역할: bald 변환용 ControlNet(잠재공간) 가중치
      - 사용처: gradio_demo_full.py의 bald_converter에 로드 → remove_hair_pipeline에서 ID 이미지를 “대머리(bald)”로 변환

Stage2
- models/stage2/pytorch_model.bin
      - 역할: 레퍼런스 인코더(ref_unet) 가중치
      - 사용처: ref_unet.from_pretrained(...).load_state_dict(...) → 참조 헤어 이미지에서 특성 추출
  - models/stage2/pytorch_model_1.bin
      - 역할: 어댑터(adapter_injection) 가중치
      - 사용처: adapter_injection(self.pipeline.unet, ...) 후 load_state_dict(...) → 인코더 특성을 UNet에 주입
  - models/stage2/pytorch_model_2.bin
      - 역할: 헤어 전이용 ControlNet(잠재공간) 가중치
      - 사용처: ControlNetModel.from_unet(unet)에 load_state_dict(...) → Stage2 생성 시 구조/컨디션 가이던스 제공



0930
StlyeGan 똑같은 pose일때 가장 안정적임
ref 헤어 포즈가 다르면... 포즈 맞추다 보면 머리 방향이 달라짐

그렇다면 ref 포즈 바꿔주는 환경에서는 diffusion이 나음

source 이미지에서는 stylegan 이 잘 되는 경우에만 적요됨
다른 포즈 일 때는 영상 자체는 StyleGAN으로 하는 게 좋고, source image 이미지가 이상할 때는 그거를 맞춰주는 과정을 diffusion 

diffusion을 쓰는 거가 

StyleGAN이 못하는게 ref pose 너무 다를 때
Diffusion 못하는게 style contest 잘 생성함. ref 헤어 입히는 게 

pose alignment를 Diffusion으로 대체?
